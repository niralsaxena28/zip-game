# Complete Explanation of zip_game.py - Visual Guide

## OVERVIEW: What Does This File Do?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         RL ZIP GAME - Complete System              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                     â”‚
â”‚  1. ZipGame â”€â”€â”€â”€â†’ Creates puzzle environment       â”‚
â”‚     â†“                                               â”‚
â”‚  2. QLearningAgent â”€â”€â”€â”€â†’ Learns to solve puzzle    â”‚
â”‚     â†“                                               â”‚
â”‚  3. AStarSolver â”€â”€â”€â”€â†’ Backup solver (if QL fails)  â”‚
â”‚     â†“                                               â”‚
â”‚  4. Interactive Game â”€â”€â”€â”€â†’ User plays puzzle       â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The file has **3 main classes + 2 helper functions + 1 main function**:
- `ZipGame`: The puzzle environment
- `QLearningAgent`: The learning AI (RL)
- `AStarSolver`: Smart backup solver
- `print_interactive_grid()`: Display function
- `play_interactive_game()`: Game loop
- `main()`: Program start

---

## SECTION 1: IMPORTS (Lines 1-4)

```python
import random                      # Line 1
import numpy as np                 # Line 2
from heapq import heappush, heappop  # Line 3
import os                          # Line 4
```

### What each import does:

| Import | Why We Need It | Used For |
|--------|---------------|----------|
| `random` | Generate randomness | Epsilon-greedy exploration, shuffling |
| `numpy as np` | Math operations | Finding max Q-value with `np.argmax()` |
| `heappush, heappop` | Priority queue | A* algorithm's efficient state management |
| `os` | Operating system commands | `os.system('cls')` to clear terminal |

---

## SECTION 2: ZIPGAME CLASS (Lines 6-35)

### Purpose: Store and manage the puzzle

```
ZipGame Class = The Environment
â”‚
â”œâ”€ self.size â”€â”€â”€â”€â”€â”€â†’ Grid dimensions (3, 4, or 5)
â”œâ”€ self.grid â”€â”€â”€â”€â”€â”€â†’ 2D array with numbers placed
â”œâ”€ self.numbers â”€â”€â”€â†’ Dictionary: {1: (0,0), 2: (1,2), ...}
â””â”€ self.solution_path â”€â”€â†’ Optimal path coordinates
```

### Code Breakdown:

```python
class ZipGame:
    def __init__(self, size):
        self.size = size                              # Line 8
        self.grid = [[0 for _ in range(size)] 
                     for _ in range(size)]            # Line 9
        self.numbers = {}                             # Line 10
        self.solution_path = []                       # Line 11
```

**Example: 3x3 grid**

```
self.size = 3

self.grid = [
    [0, 0, 0],
    [0, 0, 0],
    [0, 0, 0]
]

# After generate_game(3):
self.grid = [
    [1, 0, 2],
    [0, 3, 0],
    [0, 0, 0]
]

self.numbers = {
    1: (0, 0),  # Number 1 is at position (0,0)
    2: (0, 2),  # Number 2 is at position (0,2)
    3: (1, 1)   # Number 3 is at position (1,1)
}
```

### generate_game Method (Lines 13-29)

```python
def generate_game(self, num_count):
    # Reset grid and numbers
    self.grid = [[0 for _ in range(self.size)] 
                 for _ in range(self.size)]  # Line 15
    self.numbers = {}                        # Line 16
    
    # Create list of ALL positions
    positions = []                           # Line 19
    for i in range(self.size):
        for j in range(self.size):
            positions.append((i, j))         # Line 22
    
    # Shuffle them randomly
    random.shuffle(positions)                # Line 24
    
    # Place numbers 1 to num_count at shuffled positions
    for i in range(1, num_count + 1):       # Line 26
        pos = positions[i-1]                 # Line 27: Get a random position
        self.numbers[i] = pos                # Line 28: Store mapping
        self.grid[pos[0]][pos[1]] = i        # Line 29: Place on grid
```

**Step-by-step example for 3x3 grid with 3 numbers:**

```
STEP 1: Create all positions
positions = [(0,0), (0,1), (0,2), (1,0), (1,1), (1,2), (2,0), (2,1), (2,2)]

STEP 2: Shuffle them randomly
random.shuffle(positions)
positions = [(2,1), (0,0), (1,2), (0,1), (1,0), (2,0), (0,2), (1,1), (2,2)]

STEP 3: Place numbers 1, 2, 3 at first 3 positions
i=1: numbers[1] = (2,1)  â†’ grid[2][1] = 1
i=2: numbers[2] = (0,0)  â†’ grid[0][0] = 2
i=3: numbers[3] = (1,2)  â†’ grid[1][2] = 3

RESULT:
numbers = {1: (2,1), 2: (0,0), 3: (1,2)}

grid = [
    [2, 0, 0],
    [0, 0, 3],
    [0, 1, 0]
]
```

### print_grid Method (Lines 31-35)

```python
def print_grid(self):
    print("\nCurrent Grid:")
    for row in self.grid:
        print([f"{x:2}" if x != 0 else " ." for x in row])
    print()
```

**What it outputs:**

```
Current Grid:
[' 2', ' 0', ' 0']
[' 0', ' 0', ' 3']
[' 0', ' 1', ' 0']
```

---

## SECTION 3: QLEARNINGAGENT CLASS (Lines 37-167)

### Purpose: The AI that LEARNS to solve the puzzle

```
QLearningAgent = The Brain
â”‚
â”œâ”€ Q-Table â”€â”€â”€â”€â†’ Memory of good/bad states
â”œâ”€ Learning â”€â”€â”€â†’ Updates memory from experience
â”œâ”€ Exploration â”€â†’ Tries random moves
â””â”€ Exploitation â”€â†’ Uses learned best moves
```

### __init__ Method (Lines 40-58)

```python
def __init__(self, grid_size):
    self.grid_size = grid_size           # Line 41: Store size
    self.q_table = {}                    # Line 42: THE BRAIN
    self.learning_rate = 0.1             # Line 43: Alpha (Î±)
    self.discount = 0.9                  # Line 44: Gamma (Î³)
    self.epsilon = 0.5                   # Line 49: Explore: 50% random
    self.epsilon_min = 0.05              # Line 50: Explore: min 5% random
    self.epsilon_decay = 0.995           # Line 51: Reduce exploration slowly
```

**Visual of Q-Table Evolution:**

```
EPISODE 1 (Fresh Start):
q_table = {}  # Empty!

EPISODE 10 (Learning):
q_table = {
    state_1: 0.5,
    state_2: -0.2,
    state_3: 1.8,
}

EPISODE 3000 (Expert):
q_table = {
    state_1: 2.5,
    state_2: -5.0,
    state_3: 0.8,
    ... thousands more ...
}
```

**Epsilon Decay Graph:**

```
Exploration Probability
â”‚
â”‚ â•±â•²
â”‚â•±  â•²___________________
â”‚     Epsilon Decay
â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Episodes
0    500   1000  1500 2000 2500 3000

Episode 1:    epsilon = 0.50 (50% random moves)
Episode 500:  epsilon = 0.09 (9% random moves)
Episode 1000: epsilon = 0.05 (5% random moves - minimum reached)
```

### get_state Method (Lines 53-55)

```python
def get_state(self, pos, visited, target_num):
    return (pos, tuple(sorted(visited)), target_num)
```

**What is a STATE?**

```
State = Snapshot of puzzle situation

Example State:
(
    (2, 3),                        # Position: I'm at (2,3)
    ((0,0), (0,1), (1,2)),         # Visited: I've visited these cells
    3                              # Target: I'm trying to reach number 3
)

Meaning: "I'm at position (2,3), I've visited these 3 cells so far, 
and my goal is to reach the next target number (3)"
```

**Why tuple(sorted(visited))?**

```
visited could be a set: {(2,1), (0,0), (1,1)}

But Q-table needs HASHABLE keys (can't use sets as dict keys)

So we convert:
- Set â†’ list â†’ sorted â†’ tuple
- {(2,1), (0,0), (1,1)} â†’ ((0,0), (1,1), (2,1))

Now it's hashable and can be a dictionary key!
```

### get_reward Method (Lines 57-88)

```python
def get_reward(self, old_pos, new_pos, visited, target_num, numbers):
    # Check if revisiting
    if new_pos in visited:
        return -10  # Line 73: BIG PENALTY
    
    # Check if reached target
    if new_pos == numbers.get(target_num):
        return +20  # Line 76: BIG REWARD
    
    # Distance-based guidance
    target_pos = numbers.get(target_num)
    if target_pos:
        old_dist = abs(old_pos[0] - target_pos[0]) + abs(old_pos[1] - target_pos[1])
        new_dist = abs(new_pos[0] - target_pos[0]) + abs(new_pos[1] - target_pos[1])
        if new_dist < old_dist:
            return +5   # Line 85: Getting closer!
        else:
            return -5   # Line 87: Getting farther!
    return -1
```

**Reward System Visualized:**

```
SCENARIO 1: Try to revisit a cell
old_pos = (1, 1)
new_pos = (1, 0)  â† Already visited!
visited = {(0,0), (1,1), (1,0)}

Reward = -10  âœ— PENALTY (breaks the rule)


SCENARIO 2: Reach the target
old_pos = (1, 0)
new_pos = (2, 1)
numbers[target_num] = (2, 1)

Reward = +20  âœ“ SUCCESS!


SCENARIO 3: Move closer to target
old_pos = (0, 0), target = (3, 3)
Manhattan distance = |0-3| + |0-3| = 6

new_pos = (0, 1)
New distance = |0-3| + |1-3| = 5

5 < 6, so Reward = +5  âœ“ Good direction!


SCENARIO 4: Move away from target
old_pos = (0, 1), target = (3, 3)
Distance = 5

new_pos = (0, 0)
New distance = |0-3| + |0-3| = 6

6 > 5, so Reward = -5  âœ— Wrong direction!
```

### get_valid_moves Method (Lines 90-101)

```python
def get_valid_moves(self, pos, grid_size, visited=None):
    moves = []                                    # Line 91: Empty list
    x, y = pos                                    # Line 92: Unpack position
    directions = [(0,1), (0,-1), (1,0), (-1,0)]  # Line 93: 4 directions
    
    for dx, dy in directions:
        new_x, new_y = x + dx, y + dy            # Line 94-95: New position
        
        # Check if in bounds
        if 0 <= new_x < grid_size and 0 <= new_y < grid_size:  # Line 96
            # Check if not visited yet
            if visited is None or (new_x, new_y) not in visited:  # Line 97
                moves.append((new_x, new_y))     # Line 98: Add to valid
    
    return moves                                  # Line 99: Return list
```

**Example:**

```
Current position: (2, 3) in a 5x5 grid
visited = {(2,2), (2,3), (1,3), (2,4)}

Check 4 directions from (2,3):

1. RIGHT: (0,1)
   new_pos = (2, 3+1) = (2, 4)
   In bounds? YES. Visited? YES â†’ Skip

2. LEFT: (0,-1)
   new_pos = (2, 3-1) = (2, 2)
   In bounds? YES. Visited? YES â†’ Skip

3. DOWN: (1,0)
   new_pos = (2+1, 3) = (3, 3)
   In bounds? YES. Visited? NO â†’ ADD âœ“

4. UP: (-1,0)
   new_pos = (2-1, 3) = (1, 3)
   In bounds? YES. Visited? YES â†’ Skip

RESULT: valid_moves = [(3, 3)]
Only one valid move available!
```

### find_path Method (Lines 102-167) - THE MAIN LEARNING LOOP

This is where Q-Learning happens. Breaking it down:

```python
def find_path(self, game, max_episodes=3000):
    numbers = game.numbers                       # Line 110
    total_numbers = len(numbers)                 # Line 111
    
    # MAIN TRAINING LOOP
    for episode in range(max_episodes):          # Line 113
        # Initialize episode
        current_pos = numbers[1]                 # Line 115: Start at number 1
        visited = {current_pos}                  # Line 116: Mark as visited
        target_num = 2                           # Line 117: First target
        path = [current_pos]                     # Line 118: Track path
        
        # Try to solve this episode
        while target_num <= total_numbers:       # Line 122
            # Get current state
            state = self.get_state(current_pos, visited, target_num)  # Line 123
            
            # Get possible moves
            valid_moves = self.get_valid_moves(current_pos, game.size, visited)  # Line 124
            
            # Dead end?
            if not valid_moves:                  # Line 127
                break  # Episode fails
```

**Visual of One Episode:**

```
EPISODE 50:
â”œâ”€ Start at position of number 1: (0,0)
â”œâ”€ visited = {(0,0)}
â”œâ”€ target_num = 2
â”œâ”€
â”œâ”€ STEP 1: At (0,0), looking for target 2
â”‚  â”œâ”€ state = ((0,0), ((0,0),), 2)
â”‚  â”œâ”€ valid_moves = [(0,1), (1,0)]
â”‚  â”œâ”€ epsilon = 0.48 (still exploring)
â”‚  â”œâ”€ random.random() = 0.3, 0.3 < 0.48 â†’ EXPLORE
â”‚  â”œâ”€ next_pos = (0,1)  [random choice]
â”‚  â”œâ”€ reward = -5 (moving away from target)
â”‚  â”œâ”€ UPDATE Q-table...
â”‚  â””â”€ current_pos = (0,1), visited = {(0,0), (0,1)}
â”‚
â”œâ”€ STEP 2: At (0,1), looking for target 2
â”‚  â”œâ”€ state = ((0,1), ((0,0), (0,1)), 2)
â”‚  â”œâ”€ valid_moves = [(0,2), (1,1)]
â”‚  â”œâ”€ random.random() = 0.9, 0.9 < 0.48? NO â†’ EXPLOIT
â”‚  â”œâ”€ next_pos = best Q-value move
â”‚  â”œâ”€ reward = +20 (FOUND TARGET 2!)
â”‚  â”œâ”€ UPDATE Q-table...
â”‚  â”œâ”€ target_num = 3
â”‚  â””â”€ current_pos = (0,2), visited = {(0,0), (0,1), (0,2)}
â”‚
â”œâ”€ STEP 3: At (0,2), looking for target 3
â”‚  â”œâ”€ ... continue ...
â”‚
â””â”€ END: Episode timeout or success
```

**Continuing find_path - The Decision (Lines 132-146):**

```python
            # EPSILON-GREEDY DECISION
            if random.random() < self.epsilon:   # Line 132
                # EXPLORE: Random move
                next_pos = random.choice(valid_moves)  # Line 130
            else:
                # EXPLOIT: Use learned best move
                q_values = []                    # Line 135: List of values
                for move in valid_moves:         # Line 136: For each move
                    next_state = self.get_state(move, visited, target_num)  # Line 137
                    q_values.append(self.q_table.get(next_state, 0))  # Line 138
                
                # Pick best
                best_idx = np.argmax(q_values)   # Line 140: Index of max
                next_pos = valid_moves[best_idx] # Line 141: Use that move
```

**Example Decision:**

```
At state: ((1,1), visited_set, target=2)
valid_moves = [(1,2), (2,1), (0,1)]

epsilon = 0.15 (15% random)
random.random() = 0.08

0.08 < 0.15 â†’ EXPLORE (random move)
next_pos = random.choice([(1,2), (2,1), (0,1)])
next_pos = (2,1)  [might be a bad move]

---

DIFFERENT SCENARIO:
At state: ((1,1), visited_set, target=2)
valid_moves = [(1,2), (2,1), (0,1)]

epsilon = 0.05 (5% random)
random.random() = 0.08

0.08 < 0.05? NO â†’ EXPLOIT (use learned)

Get Q-values:
q_values = [
    self.q_table[((1,2), visited, 2)] = 2.5,
    self.q_table[((2,1), visited, 2)] = -0.8,
    self.q_table[((0,1), visited, 2)] = 0.3
]

best_idx = np.argmax([2.5, -0.8, 0.3]) = 0
next_pos = valid_moves[0] = (1,2)  [best move!]
```

**Continuing find_path - The Q-Learning Update (Lines 148-158):**

```python
            # Calculate reward
            reward = self.get_reward(current_pos, next_pos, visited, target_num, numbers)  # Line 148
            
            # Q-LEARNING UPDATE
            next_state = self.get_state(next_pos, visited, target_num)  # Line 151
            old_q = self.q_table.get(state, 0)                          # Line 152
            
            # Get best future Q-value
            next_max_q = max([
                self.q_table.get(self.get_state(move, visited | {next_pos}, target_num), 0)
                for move in self.get_valid_moves(next_pos, game.size, visited | {next_pos})
            ], default=0)  # Lines 153-154
            
            # THE BELLMAN UPDATE
            self.q_table[state] = old_q + self.learning_rate * (reward + self.discount * next_max_q - old_q)  # Line 156
```

**The Q-Learning Formula Explained:**

```
new_Q = old_Q + Î± Ã— (reward + Î³ Ã— max_future_Q - old_Q)
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   temporal_difference_error

Example:
old_Q = 0.5
reward = +5 (moving closer to target)
max_future_Q = 0.8 (best we can do from next state)
Î± = 0.1 (learning_rate)
Î³ = 0.9 (discount)

temporal_difference = 5 + 0.9 Ã— 0.8 - 0.5
                    = 5 + 0.72 - 0.5
                    = 5.22

new_Q = 0.5 + 0.1 Ã— 5.22
      = 0.5 + 0.522
      = 1.022

The Q-value for this state improved!
```

**Continuing find_path - Move and Check (Lines 159-176):**

```python
            # Actually move
            current_pos = next_pos                  # Line 159
            visited.add(current_pos)                # Line 160
            path.append(current_pos)                # Line 161
            
            # Check if reached target
            if current_pos == numbers.get(target_num):  # Line 163
                target_num += 1                     # Line 165: Next target
                
                # Check if solved completely
                if target_num > total_numbers:      # Line 166
                    if len(visited) == game.size * game.size and \
                       current_pos == numbers[total_numbers]:
                        print(f"Solution found in episode {episode + 1}!")
                        return path                 # SUCCESS!
            
            # Prevent infinite loops
            if len(path) > game.size * game.size + 10:  # Line 171
                break
        
        # EPSILON DECAY
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)  # Line 176
```

**Success Condition Check:**

```
To WIN, you need ALL of:
1. Reached all targets in order (1â†’2â†’3â†’...â†’N)
2. Visited every cell in the grid exactly once
3. Currently standing on the last target number

Example for 3x3 grid:
â”œâ”€ Path length = 9 (3Ã—3 grid)
â”œâ”€ Current position = numbers[3] (on last target)
â”œâ”€ Path covers all cells
â””â”€ Episode solved! âœ“
```

---

## SECTION 4: ASTARSOLVER CLASS (Lines 169-268)

### Purpose: Deterministic backup solver (NOT machine learning)

```
A* Solver = Logical search algorithm
â”‚
â”œâ”€ Manhattan Distance â”€â†’ Estimate distance to target
â”œâ”€ Priority Queue â”€â”€â”€â”€â”€â†’ Explore best paths first
â””â”€ Heuristic Search â”€â”€â”€â†’ Find solution smartly
```

### Key Methods:

```python
def manhattan_distance(self, pos1, pos2):
    return abs(pos1[0] - pos2[0]) + abs(pos1[1] - pos2[1])
```

**Example:**

```
From (0,0) to (3,3):
distance = |0-3| + |0-3| = 3 + 3 = 6

From (1,2) to (2,5):
distance = |1-2| + |2-5| = 1 + 3 = 4

This is "taxicab distance" - only up/down/left/right allowed
```

```python
def estimate_cost(self, pos, visited, target_num):
    if target_num > len(self.numbers):
        # All targets reached, count remaining cells
        remaining_cells = self.size * self.size - len(visited)
        return remaining_cells
    target_pos = self.numbers[target_num]
    return self.manhattan_distance(pos, target_pos)
```

**Heuristic Logic:**

```
STATE 1: Still have targets to reach
â”œâ”€ Estimate = distance to next target
â”œâ”€ Example: 4 cells away from target 3
â””â”€ Heuristic = 4

STATE 2: All targets reached
â”œâ”€ Estimate = cells still unvisited
â”œâ”€ Example: visited 7/9 cells
â””â”€ Heuristic = 2 remaining
```

```python
def solve(self, max_states=100000):
    start_pos = self.numbers[1]
    
    # Priority queue: (f_score, g_score, position, visited, target_num, path)
    heap = [(0, 0, start_pos, frozenset([start_pos]), 2, [start_pos])]
    visited_states = set()
    states_explored = 0
    
    while heap:
        states_explored += 1
        if states_explored > max_states:
            return None  # Give up
        
        # Get best state
        f_score, g_score, pos, visited, target_num, path = heappop(heap)
        
        # Check if solved
        if target_num > len(self.numbers) and len(visited) == self.size * self.size and \
           pos == self.numbers[len(self.numbers)]:
            return list(path)  # SOLUTION FOUND!
        
        # Try all neighbors
        for next_pos in self.get_neighbors(pos, visited):
            new_visited = visited | {next_pos}
            new_path = path + [next_pos]
            new_g_score = g_score + 1
            
            # Check if reached target
            new_target_num = target_num
            if target_num <= len(self.numbers) and next_pos == self.numbers[target_num]:
                new_target_num = target_num + 1
            
            # Calculate f_score
            h_score = self.estimate_cost(next_pos, new_visited, new_target_num)
            new_f_score = new_g_score + h_score
            
            # Add to heap
            heappush(heap, (new_f_score, new_g_score, next_pos, 
                           frozenset(new_visited), new_target_num, new_path))
    
    return None  # No solution
```

**A* Algorithm Visualization:**

```
Priority Queue (Heap):
Each entry: (f_score, g_score, pos, visited, target, path)

f_score = g_score + heuristic
â””â”€ Total estimated cost from START to GOAL through this node

g_score = actual steps taken so far
â””â”€ How far we've actually come

heuristic = estimated remaining cost
â””â”€ Guess of how much further

EXAMPLE:
f_score = 15
â”œâ”€ g_score = 10 (we've taken 10 steps)
â””â”€ heuristic = 5 (we estimate 5 more steps needed)

A* picks states with LOWEST f_score first
â†’ Smart search (explores most promising first)
```

---

## SECTION 5: INTERACTIVE GAME (Lines 270-534)

### print_interactive_grid (Lines 244-292)

```python
def print_interactive_grid(game, current_pos, path, visited_numbers):
    size = game.size
    
    # Show progress
    progress = ""
    for num in range(1, len(game.numbers) + 1):
        if num in visited_numbers:
            progress += f"[{num}âœ“]"
        else:
            progress += f"[{num} ]"
    
    cells_visited = len(set(path))
    total_cells = size * size
    print(f"\nProgress: {progress} | Cells: {cells_visited}/{total_cells} | Position: {current_pos}")
    
    # Draw grid with borders
    for i in range(size):
        print("+---+" * size)  # Top border
        
        row_str = "|"
        for j in range(size):
            pos = (i, j)
            
            if pos == current_pos:
                content = " @ "  # You are here
            elif pos in game.numbers.values():
                num = [k for k, v in game.numbers.items() if v == pos][0]
                if num in visited_numbers:
                    content = f"{num}âœ“ "
                else:
                    content = f" {num} "
            elif pos in path:
                content = " * "  # Been here
            else:
                content = "   "  # Empty
            
            row_str += content + "|"
        
        print(row_str)
    
    print("+---+" * size)  # Bottom border
```

**Visual Output:**

```
Progress: [1âœ“][2âœ“][3 ] | Cells: 6/9 | Position: (1,2)

+---+---+---+
| 1âœ“| * | * |
+---+---+---+
| * | 3 | @ |
+---+---+---+
| * | * | 2âœ“|
+---+---+---+

Legend:
@ = Your current position
* = You've been here
âœ“ = You've visited this number
1,2,3 = Target numbers
   = Empty space
```

### play_interactive_game (Lines 294-500)

This is the main game loop where the user plays.

```python
def play_interactive_game(game):
    print("\n" + "="*60)
    print("Now it's your turn!")
    print("="*60)
    
    # Initialize
    start_pos = game.numbers[1]
    current_pos = start_pos
    path = [current_pos]
    visited_numbers = {1}
    next_target = 2
    message = ""
    
    direction_map = {
        'u': (-1, 0),  # Up
        'd': (1, 0),   # Down
        'l': (0, -1),  # Left
        'r': (0, 1)    # Right
    }
    
    while True:
        # Clear screen
        os.system('cls' if os.name == 'nt' else 'clear')
        
        # Show grid
        print_interactive_grid(game, current_pos, path, visited_numbers)
        
        # Show message
        if message:
            print(message)
            print()
        
        # Check if won
        if len(path) == game.size * game.size and \
           current_pos == game.numbers[len(game.numbers)]:
            print("âœ¨" * 30)
            print("ğŸ‰ CONGRATULATIONS! ğŸ‰")
            print("âœ¨" * 30)
            break
        
        # Show commands
        print("Legend: @ = You | * = Visited | âœ“ = Number reached")
        print("Commands: [U]p [D]own [L]eft [R]ight | [B]ack [T]ry again [C]heck [H]elp [Q]uit")
        
        # Get input
        move = input("Your move: ").strip().lower()
        
        # Process commands...
        if move == 'h':      # Help
            # Show help
        elif move == 'b':    # Back (undo)
            if len(path) > 1:
                path.pop()
                current_pos = path[-1]
                message = "â† Moved back one step"
        elif move == 't':    # Try again
            current_pos = start_pos
            path = [current_pos]
            visited_numbers = {1}
            message = "ğŸ”„ Restarted!"
        elif move == 'c':    # Check progress
            # Show what's missing
        elif move == 'q':    # Quit
            # Show solution
            break
        elif move in direction_map:  # Movement
            dy, dx = direction_map[move]
            new_pos = (current_pos[0] + dy, current_pos[1] + dx)
            
            # Validate
            if new_pos[0] < 0 or new_pos[0] >= game.size or \
               new_pos[1] < 0 or new_pos[1] >= game.size:
                message = "âŒ Out of bounds!"
            elif new_pos in path:
                message = "âŒ Already visited!"
            else:
                # Valid move
                current_pos = new_pos
                path.append(current_pos)
                
                # Check if reached number
                if current_pos in game.numbers.values():
                    num = [k for k, v in game.numbers.items() if v == current_pos][0]
                    visited_numbers.add(num)
                    message = f"âœ¨ Reached number {num}!"
```

**Game Flow Diagram:**

```
GAME START
  â†“
Initialize: pos = number_1, path = [pos], visited_numbers = {1}
  â†“
MAIN LOOP:
  â”œâ”€ Clear screen
  â”œâ”€ Show grid
  â”œâ”€ Get user input (u/d/l/r/h/b/t/c/q)
  â”œâ”€
  â”œâ”€ If movement (u/d/l/r):
  â”‚  â”œâ”€ Calculate new_pos
  â”‚  â”œâ”€ Validate (in bounds? not visited?)
  â”‚  â”œâ”€ If valid:
  â”‚  â”‚  â”œâ”€ current_pos = new_pos
  â”‚  â”‚  â”œâ”€ path.append(new_pos)
  â”‚  â”‚  â”œâ”€ Check if reached a number
  â”‚  â”‚  â””â”€ Update visited_numbers
  â”‚  â””â”€ If invalid:
  â”‚     â””â”€ Show error message
  â”‚
  â”œâ”€ If help (h): Show commands
  â”œâ”€ If back (b): Undo last move
  â”œâ”€ If try (t): Restart
  â”œâ”€ If check (c): Show progress
  â”œâ”€ If quit (q): Show solution
  â”‚
  â”œâ”€ Check if won (all cells visited + on last number)
  â”‚  â”œâ”€ If YES â†’ Show congratulations â†’ Break
  â”‚  â””â”€ If NO â†’ Continue loop
  â”‚
  â””â”€ Loop back to show grid

GAME END
  â†“
Exit
```

---

## SECTION 6: MAIN FUNCTION (Lines 502-594)

```python
def main():
    print("=== RL Zip Game ===")
    
    # Get player name
    name = input("Enter your name: ")
    
    # Get grid size
    print(f"\nGrid size must be between 3x3 and 5x5")
    while True:
        size = int(input(f"Enter grid size (3-5): "))
        if 3 <= size <= 5:
            break
    
    # Get number count
    recommendations = {
        3: (2, 5, 3),   # min, max, recommended
        4: (2, 6, 4),
        5: (2, 6, 4),
    }
    
    min_nums, max_nums, recommended = recommendations.get(size, (2, size, size // 2))
    
    while True:
        num_count = int(input(f"Enter number of numbers ({min_nums}-{max_nums}): "))
        if min_nums <= num_count <= max_nums:
            break
    
    # Create game and agent
    game = ZipGame(size)
    agent = QLearningAgent(size)
    
    # Generate solvable game
    attempts = 0
    max_attempts = 30
    
    while attempts < max_attempts:
        attempts += 1
        game.generate_game(num_count)
        game.print_grid()
        
        print(f"Attempt {attempts}: Training RL agent...")
        solution = agent.find_path(game, max_episodes=2000)
        
        if solution:
            print("âœ“ RL agent found solution!")
            game.solution_path = solution
            break
        else:
            print("  RL couldn't solve it, trying A*...")
            astar = AStarSolver(game)
            solution = astar.solve()
            
            if solution:
                print("âœ“ A* found solution!")
                game.solution_path = solution
                break
    
    if not game.solution_path:
        print(f"âŒ Could not generate solvable game after {max_attempts} attempts")
        return
    
    # Let user play
    play_interactive_game(game)

if __name__ == "__main__":
    main()
```

**Main Function Flow:**

```
START main()
  â†“
Get player name
  â†“
Get grid size (3-5)
  â†“
Get number count (difficulty)
  â†“
Create ZipGame object
  â†“
Create QLearningAgent object
  â†“
GENERATION LOOP (up to 30 attempts):
  â”œâ”€ Generate random puzzle
  â”œâ”€ Train Q-Learning agent (2000 episodes)
  â”œâ”€ Did it find solution? 
  â”‚  â”œâ”€ YES â†’ Store solution, BREAK
  â”‚  â””â”€ NO â†’ Try A* algorithm
  â”‚         â”œâ”€ Did A* find solution?
  â”‚         â”‚  â”œâ”€ YES â†’ Store solution, BREAK
  â”‚         â”‚  â””â”€ NO â†’ Try again (next iteration)
  â”‚         â””â”€
  â””â”€
  â†“
If no solution found after 30 attempts:
  â”œâ”€ Print error
  â””â”€ Exit
  â†“
Launch interactive game:
  â””â”€ play_interactive_game(game)
  â†“
END
```

---

## COMPLETE SYSTEM ARCHITECTURE

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RL ZIP GAME SYSTEM                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  1. PUZZLE GENERATION                                      â”‚
â”‚     â””â”€ ZipGame.generate_game()                             â”‚
â”‚        â””â”€ Creates random puzzle with N numbered targets    â”‚
â”‚                                                            â”‚
â”‚  2. SOLUTION FINDING (Two approaches)                      â”‚
â”‚     â”œâ”€ Try Q-Learning:                                     â”‚
â”‚     â”‚  â””â”€ QLearningAgent.find_path()                       â”‚
â”‚     â”‚     â””â”€ Learns through 3000 episodes of trial-error   â”‚
â”‚     â”‚                                                      â”‚
â”‚     â””â”€ If QL fails, Try A*:                                â”‚
â”‚        â””â”€ AStarSolver.solve()                              â”‚
â”‚           â””â”€ Deterministic smart search                    â”‚
â”‚                                                            â”‚
â”‚  3. USER GAMEPLAY                                          â”‚
â”‚     â””â”€ play_interactive_game()                             â”‚
â”‚        â”œâ”€ Display grid                                     â”‚
â”‚        â”œâ”€ Get user input                                   â”‚
â”‚        â”œâ”€ Validate moves                                   â”‚
â”‚        â””â”€ Check win condition                              â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## KEY CONCEPTS SUMMARY

### 1. **Q-LEARNING** (The Learning Algorithm)

```
Formula: Q(s,a) â† Q(s,a) + Î±(r + Î³Â·max_Q(s',a') - Q(s,a))

Components:
â”œâ”€ state (s): Situation in game
â”œâ”€ action (a): Move up/down/left/right
â”œâ”€ reward (r): Feedback (+20, -5, etc.)
â”œâ”€ Q-value: Quality of state-action pair
â”œâ”€ Î± (learning_rate): How much to learn (0.1)
â”œâ”€ Î³ (discount): How much future matters (0.9)
â””â”€ max_Q(s',a'): Best value from next state
```

### 2. **EPSILON-GREEDY** (Exploration vs Exploitation)

```
With probability Îµ: Explore (random move)
With probability 1-Îµ: Exploit (use Q-table)

Decay schedule:
Episode 1:    Îµ = 0.50 (50% explore)
Episode 500:  Îµ = 0.09 (9% explore)
Episode 1000: Îµ = 0.05 (5% explore - minimum)
```

### 3. **REWARD SHAPING** (Guidance System)

```
+20: Found target (goal)
+5:  Moving closer (progress)
-5:  Moving away (wrong direction)
-10: Revisited cell (rule break)
```

### 4. **STATE SPACE** (Representation)

```
state = (current_position, visited_cells, target_number)

For 3x3 grid:
â”œâ”€ Positions: 9
â”œâ”€ Visited combinations: 2^9 = 512
â”œâ”€ Targets: 3
â””â”€ Total states: ~13,824

For 5x5 grid:
â”œâ”€ Positions: 25
â”œâ”€ Visited combinations: 2^25 â‰ˆ 33 million
â”œâ”€ Targets: 6
â””â”€ Total states: 4.95 billion (!!)
```

### 5. **A* ALGORITHM** (Deterministic Backup)

```
f_score = g_score + heuristic
â”œâ”€ g_score: Steps actually taken
â”œâ”€ heuristic: Estimated remaining steps
â””â”€ Picks state with lowest f_score

Always finds solution if one exists
(but doesn't "learn" like Q-Learning)
```

---

## EXECUTION FLOW EXAMPLE

```
$ python zip_game.py

â†“ Output: "=== RL Zip Game ==="

You input: "Alice" (name)
You input: "3" (3x3 grid)
You input: "3" (3 target numbers)

â†“ System generates game:

Current Grid:
[' 2', ' 0', ' 0']
[' 0', ' 3', ' 0']
[' 0', ' 0', ' 1']

â†“ System trains RL agent (2000 episodes):

Attempt 1: Training RL agent to find solution...
Solution found in episode 1847!
âœ“ RL agent found solution!
Path length: 9

â†“ System launches interactive game:

Progress: [1âœ“][2 ][3 ] | Cells: 1/9 | Position: (2,2)

+---+---+---+
| 2 | 0 | 0 |
+---+---+---+
| 0 | 3 | 0 |
+---+---+---+
| 0 | 0 | @ |
+---+---+---+

Legend: @ = You | * = Visited | âœ“ = Number reached
Commands: [U]p [D]own [L]eft [R]ight | [B]ack [T]ry again [C]heck [H]elp [Q]uit

Your move: r

[Screen clears and updates]

Progress: [1âœ“][2 ][3 ][4 ][5âœ“] | Cells: 2/9 | Position: (2,1)

... game continues until user wins or quits ...
```

---

## SECTION 7: STREAMLIT WEB UI (streamlit_app.py)

After building the command-line version, we added a **graphical web interface** using [Streamlit](https://streamlit.io/). This new file, `streamlit_app.py`, does **not** change the core algorithms â€“ it simply provides a nicer way for humans to interact with the same `ZipGame`, `QLearningAgent`, and `AStarSolver`.

### 7.1 High-Level Overview

```text
User (Browser)
   â†“
Streamlit App (streamlit_app.py)
   â†“
ZipGame / QLearningAgent / AStarSolver (zip_game.py)
   â†“
Solution Path + Game State
```

- All puzzle generation and solving still happens in `zip_game.py`.
- `streamlit_app.py` imports those classes and exposes them via a web page.
- Streamlit automatically reruns the script on every user interaction; we use `st.session_state` to remember the current game state between reruns.

### 7.2 Session State: Remembering the Game in the Browser

At the top of `streamlit_app.py`, the function `init_session_state()` creates keys in `st.session_state`:

- `game`: the current `ZipGame` instance.
- `game_ready`: whether a solvable puzzle has been generated.
- `current_pos`: the player's current position `(row, col)`.
- `path`: list of all positions visited so far.
- `visited_numbers`: set of numbers the player has reached.
- `next_target`: the next number the player is supposed to reach.
- `message`: feedback for the player (strings like "Nice move!" or warnings).
- `message_type`: how to display the message (`info`, `warning`, `error`, `success`).
- `show_solution`: whether to display the AI's optimal path.
- `celebrated`: a flag to ensure the balloon animation runs only once per win.

Because Streamlit re-executes the script whenever a button is pressed, `session_state` is the "memory" that survives from one click to the next.

### 7.3 Generating a Solvable Puzzle (generate_solvable_game)

`generate_solvable_game(size, num_count)` performs almost the same steps as `main()` in `zip_game.py`:

1. Creates `ZipGame(size)` and `QLearningAgent(size)`.
2. Tries up to 30 random grids.
3. For each grid:
   - First calls `agent.find_path(...)` (Q-Learning).
   - If that fails, creates `AStarSolver(game)` and calls `solve()`.
4. If a solution is found, it is stored in `game.solution_path`.
5. The player's starting state (`current_pos`, `path`, `visited_numbers`, `next_target`) is initialized in `session_state`.

If **no** solution is found after 30 attempts, Streamlit shows an error message similar to the terminal version (suggesting smaller grids or fewer numbers).

### 7.4 Rendering the Grid in HTML (render_grid)

Instead of ASCII art, the web UI uses an **HTML table** with CSS styling:

- Each cell is a `<td>` element, given CSS classes like:
  - `cell-current` â€“ current player position (`@`, highlighted yellow)
  - `cell-number` â€“ numbered targets (light blue)
  - `cell-number-visited` â€“ targets already reached (green, with `âœ“`)
  - `cell-path` â€“ non-number cells that have been visited (gray)
- The table is centered and each cell is larger (e.g. `60Ã—60px`) so it looks like a small board.
- Above the grid, a centered status line shows:
  - Which numbers `1â€¦N` have been reached.
  - How many cells have been visited out of the total.
  - The current coordinate.

This directly mirrors what `print_interactive_grid()` showed in the terminal, but in a more visual, colorful form.

### 7.5 Movement and Game Logic in the Web UI

The following helper functions manage the game from the browser:

- `move_player(dy, dx)`
  - Computes the new position.
  - Checks bounds and whether the cell was already visited.
  - Updates `current_pos`, `path`, `visited_numbers`, and `next_target`.
  - Sets `message` and `message_type` depending on what happened:
    - Warning if moving out of bounds or revisiting a cell.
    - Success if the correct next number was reached.
    - Warning if the player hit a number out of order.
    - Info when moving on an empty cell.

- `step_back()`
  - Pops the last position from `path` (if possible).
  - Recomputes `visited_numbers` and `next_target` from the remaining path.
  - Updates `current_pos` to the new last cell.

- `reset_player()`
  - Resets the game back to the starting position (number 1) while keeping the same puzzle.

- `check_progress()`
  - Shows a small report in Streamlit: how many numbers and cells are visited, and whether the player is on the last number.

- `show_solution()`
  - Displays the `game.solution_path` and its length, so the player can study the optimal route.

All these functions operate on `st.session_state`, so the state changes are preserved after each button click.

### 7.6 Layout and Controls in main()

The `main()` function in `streamlit_app.py` is responsible for building the page layout:

1. **Styling**
   - Injects a small CSS `<style>` block to make Streamlit buttons look more like game controls (blue gradient, rounded corners, shadow).

2. **Sidebar (Game Settings)**
   - Sliders for **grid size** and **number of numbers**.
   - A "Generate new puzzle" button that calls `generate_solvable_game(...)`.

3. **Main Area**
   - Uses three containers:
     - `board_container`: shows the grid.
     - `message_container`: shows feedback messages (using `st.info`, `st.warning`, etc.).
     - `controls_container`: shows the control buttons.
   - Control layout:
     - A D-pad style layout for `Up`, `Down`, `Left`, `Right`.
     - A **Back** button to undo one move.
     - A **Reset** button spanning the width.
     - Below, buttons for **Check progress** and **Show optimal solution**.

4. **Win Condition and Balloons**
   - After each interaction, `main()` checks if:
     - All cells have been visited, and
     - The current position is the last numbered cell.
   - If so, it:
     - Triggers `st.balloons()` once (using the `celebrated` flag).
     - Shows a success message with the player's path length vs. the AI's.
     - Displays a more detailed comment depending on whether the player matched the optimal path exactly, matched the optimal **length** with a different route, or took a longer path.

### 7.7 Summary: CLI vs Web UI

- **Same core logic**: both interfaces rely on the same `ZipGame`, `QLearningAgent`, and `AStarSolver` code.
- **Different presentation**:
  - CLI uses ASCII art, keyboard commands, and printed text.
  - Web UI uses a colored HTML grid, clickable buttons, and browser animations.
- The Streamlit app turns the project into a small browser game without changing the underlying AI.

---

## YOU NOW UNDERSTAND THE ENTIRE CODE!

You can now:
âœ“ Explain what each class in `zip_game.py` does
âœ“ Understand the Q-Learning formula and how the Q-table is updated
âœ“ Explain the epsilon-greedy exploration strategy
âœ“ Understand the reward shaping used to guide the agent
âœ“ Trace through a full training episode
âœ“ Explain how the A* algorithm is used as a deterministic backup
âœ“ Understand the interactive **terminal** game loop
âœ“ Understand how the **Streamlit web UI** wraps the same logic for the browser
âœ“ Answer questions about both the AI design and the user interfaces

**Key takeaway:** This project demonstrates how an AI agent learns to solve puzzles through experience (Q-Learning), with a smart backup solver (A*), and exposes the same puzzle to humans via both a terminal interface and a modern Streamlit web UI!
